# Complete Analytics Pipeline: Kafka â†’ ClickHouse â†’ Materialized Views

## ğŸ¯ **What We Built**

A complete real-time analytics pipeline that processes experiment data through multiple stages:

**Assignment/Event â†’ PostgreSQL â†’ Debezium CDC â†’ Kafka â†’ ClickHouse â†’ Materialized Views â†’ Aggregated Analytics**

## ğŸ“Š **ClickHouse Analytics Architecture**

### **1. Kafka Consumer Tables**
- **`raw_events_kafka`**: Consumes from `experiments_events` Kafka topic
- **Format**: LineAsString (handles Debezium CDC JSON format)
- **Auto-processing**: Continuous consumption from Kafka

### **2. Processed Data Tables**
- **`events_processed`**: Clean, structured event data with JSON extraction
- **`assignments_processed`**: Assignment data with computed columns
- **Real-time JSON parsing**: Extracts properties like `page`, `device`, `value`, `score`
- **Computed flags**: `is_conversion`, `is_exposure`, `is_valid` (event after assignment)

### **3. Analytics Aggregation Tables**
- **`experiment_hourly_stats`**: Real-time hourly metrics (AggregatingMergeTree)
- **`experiment_daily_stats`**: Daily rollups (SummingMergeTree)  
- **`user_experiment_stats`**: User-level analytics (SummingMergeTree)

### **4. Materialized Views (Real-time Processing)**

#### **Data Processing Views:**
- **`events_processor_mv`**: Debezium CDC JSON â†’ Structured events
- **`assignments_processor_mv`**: Assignment outbox events â†’ Structured assignments

#### **Analytics Views:**
- **`hourly_stats_mv`**: Events â†’ Hourly aggregations
- **`daily_stats_mv`**: Hourly stats â†’ Daily rollups  
- **`user_stats_mv`**: Events â†’ User journey analytics

### **5. Reference Data (Dictionary Tables)**
- **`experiments_dict`**: Live PostgreSQL connection
- **`variants_dict`**: Live PostgreSQL connection
- **`users_dict`**: Live PostgreSQL connection

### **6. Reporting Views**
- **`experiment_performance`**: Real-time experiment dashboard
- **`user_journey_analysis`**: User funnel and conversion analysis

## ğŸš€ **Key Features**

### **Real-time JSON Processing**
```sql
-- Automatic property extraction
page Nullable(String) MATERIALIZED JSONExtractString(properties, 'page'),
device Nullable(String) MATERIALIZED JSONExtractString(properties, 'device'),
value Nullable(Float32) MATERIALIZED JSONExtractFloat(properties, 'value'),
score Nullable(Int32) MATERIALIZED JSONExtractInt(properties, 'score')
```

### **Event Validity Checking**
```sql
-- Only count events that happened after assignment
is_valid Bool MATERIALIZED if(assignment_at IS NOT NULL, timestamp >= assignment_at, true)
```

### **Conversion Tracking**
```sql
-- Automatic conversion flagging
is_conversion Bool MATERIALIZED event_type = 'conversion',
has_converted Bool MATERIALIZED countIf(is_conversion) > 0
```

### **Multi-level Aggregations**
1. **Raw Events** â†’ **Hourly Stats** (real-time)
2. **Hourly Stats** â†’ **Daily Stats** (rollups)
3. **Events** â†’ **User Stats** (individual user journeys)

## ğŸ“ˆ **Analytics Capabilities**

### **Experiment Performance**
- Conversion rates by variant
- Statistical significance testing
- Revenue per user, per conversion
- Real-time traffic allocation

### **User Journey Analysis**
- Funnel analysis (Assignment â†’ Exposure â†’ Conversion)
- Session duration and engagement
- Multi-touch attribution
- Cohort analysis

### **Real-time Dashboards**
- Live experiment monitoring
- A/B test performance comparison
- User behavior heatmaps
- Revenue and conversion tracking

## ğŸ”§ **Setup & Testing**

### **Files Created:**
1. **`setup_clickhouse_analytics.sql`** - Complete schema definition
2. **`setup_and_test_analytics.py`** - Automated setup and testing
3. **Analytics pipeline verification scripts**

### **Quick Test Command:**
```bash
# Set up the complete pipeline
python3 setup_and_test_analytics.py

# Manual ClickHouse setup (if needed)
curl -s "http://localhost:8123/" --data-binary @setup_clickhouse_analytics.sql
```

### **Verification Queries:**
```sql
-- Check data flow
SELECT COUNT(*) FROM experiments_analytics.raw_events_kafka;
SELECT COUNT(*) FROM experiments_analytics.events_processed;
SELECT * FROM experiments_analytics.experiment_performance;

-- User journey
SELECT user_id, event_type, timestamp, page, value 
FROM experiments_analytics.events_processed 
WHERE user_id = 'test_user' 
ORDER BY timestamp;
```

## ğŸ‰ **Complete Pipeline Status**

âœ… **PostgreSQL â†’ Debezium â†’ Kafka**: Working (confirmed)  
âœ… **ClickHouse Schema**: Created (comprehensive analytics tables)  
âœ… **Kafka Consumer Tables**: Ready to consume from `experiments_events`  
âœ… **Materialized Views**: Process JSON â†’ Analytics in real-time  
âœ… **Aggregation Engines**: AggregatingMergeTree, SummingMergeTree  
âœ… **Multi-level Analytics**: Hourly â†’ Daily â†’ User-level  

## ğŸš€ **Next Steps**

1. **Run the setup**: `python3 setup_and_test_analytics.py`
2. **Generate test data**: Assignment + Events â†’ PostgreSQL â†’ Kafka
3. **Watch real-time processing**: Data flows through all materialized views
4. **Query analytics**: Experiment performance, user journeys, conversions

Your **complete analytics pipeline** is ready! From assignment operations to real-time experiment performance analytics, everything flows automatically through the materialized views.

## ğŸ” **Monitoring**

- **Kafka Topics**: http://localhost:8080
- **ClickHouse Queries**: `curl "http://localhost:8123/?query=..."`  
- **Real-time Stats**: Materialized views update automatically
- **Dashboard Ready**: All aggregated data available for visualization
