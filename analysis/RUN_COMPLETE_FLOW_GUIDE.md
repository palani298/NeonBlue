# ğŸš€ Complete Flow Execution Guide

## ğŸ“ **Docker Files Location & Setup**

### **Docker Configuration Files**
```
config/
â”œâ”€â”€ ğŸ“„ docker-compose.yml          # Main orchestration file (8 services)
â”œâ”€â”€ ğŸ“„ Dockerfile                  # FastAPI application container
â”œâ”€â”€ ğŸ“„ prometheus.yml              # Monitoring configuration
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â”œâ”€â”€ ğŸ“ init/                       # Database initialization
â”‚   â”œâ”€â”€ postgres/                  # PostgreSQL setup scripts
â”‚   â””â”€â”€ clickhouse/               # ClickHouse setup scripts
â””â”€â”€ ğŸ“ grafana/                    # Dashboard configurations
```

### **Complete Service Stack**
```
ğŸ³ 8 Docker Services:
â”œâ”€â”€ api                  # FastAPI application (port 8000)
â”œâ”€â”€ postgres            # PostgreSQL database (port 5432)
â”œâ”€â”€ redis               # Caching layer (port 6379)
â”œâ”€â”€ kafka               # Event streaming (port 9092)
â”œâ”€â”€ debezium            # Change Data Capture (port 8083)
â”œâ”€â”€ clickhouse          # Analytics database (port 8123)
â”œâ”€â”€ grafana             # Monitoring dashboards (port 3000)
â””â”€â”€ prometheus          # Metrics collection (port 9090)
```

---

## âš¡ **Quick Start: Run Complete Flow**

### **ğŸ¯ One-Command Startup**
```bash
# From project root
./scripts/start_services.sh
```

### **ğŸ”„ Manual Step-by-Step**
```bash
# 1. Start Docker services
cd config
docker compose up -d

# 2. Wait for services to be ready (2-3 minutes)
docker compose logs -f api

# 3. Set up CDC pipeline
cd ..
./scripts/setup_cdc_pipeline_v2.sh

# 4. Run complete end-to-end test
python tests/end-to-end/test_end_to_end_flow.py
```

---

## ğŸ§ª **Testing the Complete Flow**

### **Available Test Scripts**
```
tests/
â”œâ”€â”€ end-to-end/
â”‚   â”œâ”€â”€ test_end_to_end_flow.py      # ğŸ¯ Main complete flow test
â”‚   â””â”€â”€ test_complete_flow.py        # Alternative flow test
â””â”€â”€ integration/
    â”œâ”€â”€ verify_setup.py              # âœ… Quick setup verification
    â”œâ”€â”€ setup_and_test_analytics.py  # ğŸ“Š Analytics pipeline test
    â””â”€â”€ direct_db_test.py            # ğŸ’¾ Direct database test
```

### **ğŸ¯ Run Complete End-to-End Test**
```bash
# Test: PostgreSQL â†’ Debezium CDC â†’ Kafka â†’ ClickHouse
python tests/end-to-end/test_end_to_end_flow.py

# Expected output:
# âœ… User Created: e2e_test_user_xxxxx
# âœ… Assignment Created: ID 7
# âœ… Events Created: 4 events
# âœ… Kafka Pipeline: SUCCESS
# âœ… ClickHouse Analytics: SUCCESS
```

### **ğŸ“Š Verify Setup**
```bash
# Quick health check
python tests/integration/verify_setup.py

# Expected output:
# âœ… PostgreSQL: Connected
# âœ… Redis: Connected  
# âœ… ClickHouse: Connected
# âœ… API: Running
# âœ… All services healthy
```

---

## ğŸ”§ **Troubleshooting Common Issues**

### **Issue 1: Docker Not Running**
```bash
# Error: Cannot connect to Docker daemon
# Solution:
open -a Docker  # macOS
# Wait for Docker Desktop to start, then retry
```

### **Issue 2: Port Conflicts**
```bash
# Error: Port 8000 already in use
# Solution: Stop conflicting services
lsof -ti:8000 | xargs kill -9
docker compose down
docker compose up -d
```

### **Issue 3: Services Not Ready**
```bash
# Check service health
docker compose ps
docker compose logs api
docker compose logs postgres

# Wait for all services to show "healthy"
```

### **Issue 4: CDC Pipeline Issues**
```bash
# Check Debezium connector status
curl http://localhost:8083/connectors

# Reset if needed
./scripts/setup_cdc_pipeline_v2.sh
```

### **Issue 5: ClickHouse Not Processing**
```bash
# Check ClickHouse connection
curl http://localhost:8123/

# Restart Kafka consumer
curl -s "http://localhost:8123/" --data "DETACH TABLE experiments_analytics.raw_events_kafka"
curl -s "http://localhost:8123/" --data "ATTACH TABLE experiments_analytics.raw_events_kafka"
```

---

## ğŸ“Š **Accessing Services After Startup**

### **ğŸŒ Web Interfaces**
```bash
# FastAPI Application & Docs
open http://localhost:8000
open http://localhost:8000/docs

# Grafana Dashboards
open http://localhost:3000
# Login: admin/admin

# Kafka UI
open http://localhost:8080

# ClickHouse Interface
curl "http://localhost:8123/"
```

### **ğŸ’¾ Database Connections**
```bash
# PostgreSQL
docker exec -it experiments-postgres psql -U experiments -d experiments

# ClickHouse
docker exec -it experiments-clickhouse clickhouse-client

# Redis
docker exec -it experiments-redis redis-cli
```

---

## ğŸ¯ **Complete Flow Validation**

### **Step 1: Verify All Services**
```bash
./scripts/start_services.sh
python tests/integration/verify_setup.py
```

### **Step 2: Test End-to-End Pipeline**
```bash
python tests/end-to-end/test_end_to_end_flow.py
```

### **Step 3: Check Data Flow**
```bash
# Check PostgreSQL
docker exec experiments-postgres psql -U experiments -d experiments -c "SELECT COUNT(*) FROM assignments"

# Check Kafka
docker exec experiments-kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic experiments_events --max-messages 1 --timeout-ms 3000

# Check ClickHouse
curl -s "http://localhost:8123/" --data "SELECT COUNT(*) FROM experiments_analytics.events_processed"
```

### **Step 4: View Analytics**
```bash
# Open Grafana dashboards
open http://localhost:3000

# Query ClickHouse analytics
curl -s "http://localhost:8123/" --data "
SELECT 
    variant_key,
    count() as events,
    uniq(user_id) as users 
FROM experiments_analytics.events_processed 
GROUP BY variant_key 
FORMAT Pretty"
```

---

## ğŸ”„ **Restart/Reset Instructions**

### **ğŸ”„ Restart Everything**
```bash
cd config
docker compose down
docker compose up -d
./setup_cdc_pipeline_v2.sh
```

### **ğŸ—‘ï¸ Clean Reset (Remove All Data)**
```bash
cd config
docker compose down -v  # Remove volumes
docker system prune -f  # Clean up
docker compose up -d
./setup_cdc_pipeline_v2.sh
```

### **ğŸ”§ Partial Restart (Specific Services)**
```bash
# Restart just the API
docker compose restart api

# Restart analytics pipeline
docker compose restart clickhouse debezium kafka

# Rebuild API container
docker compose build api
docker compose up -d api
```

---

## ğŸ“ˆ **Performance Monitoring**

### **Real-time Monitoring**
```bash
# Watch Docker logs
docker compose logs -f api

# Monitor resource usage
docker stats

# Check service health
watch -n 5 'docker compose ps'
```

### **Analytics Performance**
```bash
# Query performance metrics
curl -s "http://localhost:8123/" --data "
SELECT 
    'Events processed' as metric,
    COUNT(*) as value 
FROM experiments_analytics.events_processed
UNION ALL
SELECT 
    'Assignments created' as metric,
    COUNT(*) as value 
FROM assignments
FORMAT Pretty"
```

---

## âœ… **Success Indicators**

### **ğŸ¯ Complete Flow Working When:**
- âœ… All 8 Docker containers running
- âœ… API responds at http://localhost:8000/health
- âœ… Grafana dashboards load at http://localhost:3000
- âœ… End-to-end test passes completely
- âœ… Data flows: PostgreSQL â†’ Kafka â†’ ClickHouse
- âœ… Analytics queries return data

### **ğŸ“Š Test Results Should Show:**
```
ğŸ¯ END-TO-END TEST SUMMARY
==================================================
âœ… User Created: e2e_test_user_xxxxx
âœ… Assignment Created: ID 7
âœ… Events Created: 4 events
âœ… Kafka Pipeline: SUCCESS
âœ… ClickHouse Analytics: SUCCESS

ğŸ‰ OVERALL RESULT: SUCCESS
```

---

## ğŸ‰ **Ready to Run!**

The NeonBlue platform is **fully configured and ready to run** with:

âœ… **Complete Docker setup** in `config/`  
âœ… **One-command startup** with `./scripts/start_services.sh`  
âœ… **Comprehensive testing** with end-to-end validation  
âœ… **Production monitoring** with Grafana + Prometheus  
âœ… **Real-time analytics** with ClickHouse pipeline  

**Just run `./scripts/start_services.sh` and you're ready to go!** ğŸš€

---

**ğŸ”— Next Steps:**
1. Start services: `./scripts/start_services.sh`
2. Test flow: `python tests/end-to-end/test_end_to_end_flow.py`
3. View analytics: http://localhost:3000
4. Experiment with API: http://localhost:8000/docs
